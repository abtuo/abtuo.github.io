<p>I will share here some literature articles related to my research and also some interesting AI and NLP related resources.</p>
<h3 id="table-of-content">Table of content</h3>

<ul>
  <li><a href="#event-extraction">Event extraction</a></li>
  <li><a href="#domain-adaptation">Domain adaptation</a></li>
  <li><a href="#few-shot-learning">Few-shot learning</a></li>
  <li><a href="#named-entity-recognition">Named entity recognition</a></li>
  <li><a href="#datasets">Datasets</a></li>
  <li><a href="#semantic-role-labelling">Semantic role labelling</a></li>
</ul>

<h3 id="event-extraction">Event extraction</h3>

<ul>
  <li><a href="http://arxiv.org/abs/1805.07091">Tropical Geometry of Deep Neural Networks</a></li>
</ul>

<p><i>Zhang, Liwen and Naitzat, Gregory and Lim, Lek-Heng  2018</i>&lt;/summary&gt;</p>
<blockquote>
  <p>We establish, for the ﬁrst time, connections between feedforward neural networks with ReLU activation and tropical geometry — we show that the family of such neural networks is equivalent to the family of tropical rational maps. Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions. An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.</p>
</blockquote>

<p>in arXiv:1805.07091 [cs, math, stat]</p>
<ul>
  <li><a href="http://arxiv.org/abs/1904.05046">Generalizing from a Few Examples: A Survey on Few-Shot Learning</a></li>
</ul>

<p><i>Wang, Yaqing and Yao, Quanming and Kwok, James and Ni, Lionel M.  2020</i>&lt;/summary&gt;</p>
<blockquote>
  <p>Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.</p>
</blockquote>

