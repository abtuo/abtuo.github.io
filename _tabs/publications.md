---
title: Publications
icon: fas fa-stream
order: 2
---

<details>
<summary><b style="font-family: 'Arial'; color: blue; font-size: 24px;"> 2024 </b></summary>
<!-- Ajoutez les publications de 2023 ici -->
</details>


<details>
<summary><b style="font-family: 'Arial'; color: blue; font-size: 24px;"> 2023 </b></summary>
<!-- Ajoutez les publications de 2023 ici -->
</details>

<details>
<summary><b style="font-family: 'Arial'; color: blue; font-size: 24px;"> 2022 </b></summary>
<details>
<summary> <a href="https://link.springer.com/chapter/10.1007/978-3-031-08473-7_26"><b>Better Exploiting BERT for Few-shot Event Detection</b></a></summary>
<p><i>Aboubacar Tuo, Romaric Besançon, Olivier Ferret, Julien Tourille. NLDB, 2022.</i></p>

<blockquote>
Recent approaches for event detection rely on deep supervised learning, which requires large annotated corpora. Few-shot learning approaches, such as the meta-learning paradigm, can be used to address this issue. We focus in this paper on the use of prototypical networks with a BERT encoder for event detection. More specifically, we optimize the use of the information contained in the different layers of a pre-trained BERT model and show that simple strategies for combining BERT layers can outperform the current state-of-the-art for this task.
</blockquote>

<p><a href="mailto:aboubacar.tuo@cea.fr">ask me for free version</a></p>
</details>
<details>
<summary> <a href="https://hal.archives-ouvertes.fr/hal-03701491/file/3792.pdf"><b>Mieux utiliser BERT pour la détection d’évènements à partir de peu d’exemples</b></a></summary>
<p><i>Aboubacar Tuo, Romaric Besançon, Olivier Ferret, Julien tourille. TALN, 2022.</i></p>

<blockquote>
Les méthodes actuelles pour la détection d’évènements, qui s’appuient essentiellement sur l’apprentissage supervisé profond, s’avèrent très coûteuses en données annotées. Parmi les approches pourl’apprentissage à partir de peu de données, nous exploitons dans cet article le méta-apprentissage et l’utilisation de l’encodeur BERT pour cette tâche. Plus particulièrement, nous explorons plusieurs stratégies pour mieux exploiter les informations présentes dans les différentes couches d’un modèle BERT pré-entraîné et montrons que ces stratégies simples permettent de dépasser les résultats de l’état de l’art pour cette tâche en anglais.
</blockquote>
</details>

</details>